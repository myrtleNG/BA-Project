# Class Project 3 â€“ NYC 311 Calls
NYC 311 is a service that provides access to non-emergency City services and info about City government programs to the residents of New York. Each year, the service receives millions of requests reporting various kinds of problems with city services and other issues.
The data on the type of calls received and their ultimate resolution is made available through the NYC Open Data portal at https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9. The data is updated daily. The link also provides the data dictionary for the data.
To ensure that using the same data and arrive at the same results, the data has been downloaded and includes information up to 2023-08-04 12:00:00. Several columns not required for this project have been removed from the original data.


import pandas as pd

# Importing the NYC 311 service request data from the pickle file
df = pd.read_pickle('/shared/Project-3_NYC_311_Calls.pkl')

# Basic exploratory data analysis
# Display the first few rows of the dataframe
df.head()


import pandas as pd

# Importing the NYC 311 service request data from the pickle file
df = pd.read_pickle('Project-3_NYC_311_Calls.pkl')

# Basic exploratory data analysis
# Display the first few rows of the dataframe
df.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the NYC 311 service request data into a DataFrame (assuming 'df' is your DataFrame)
df = pd.read_pickle('Project-3_NYC_311_Calls.pkl')

# Convert 'Created Date' column to datetime and set it as the index
df['Created Date'] = pd.to_datetime(df['Created Date'])
df.set_index('Created Date', inplace=True)

# Display basic information about the dataset
print(df.info())

# Display the first few rows of the dataframe
print(df.head())

# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

# Descriptive statistics for numerical columns
print(df.describe())

# Plot a bar chart of the top 10 complaint types
plt.figure(figsize=(12, 6))
df['Complaint Type'].value_counts().nlargest(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Complaint Types')
plt.xlabel('Complaint Type')
plt.ylabel('Count')
plt.show()

# Plot a time series of service requests over time (monthly)
plt.figure(figsize=(12, 6))
df.resample('M').size().plot()
plt.title('Monthly Service Requests Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Requests')
plt.show()

# Analyze the distribution of requests by borough
plt.figure(figsize=(10, 6))
sns.countplot(x='Borough', data=df, order=df['Borough'].value_counts().index, palette='viridis')
plt.title('Distribution of Requests by Borough')
plt.xlabel('Borough')
plt.ylabel('Number of Requests')
plt.xticks(rotation=45)
plt.show()

## Average number of daily complaints received in 2022

import pandas as pd

# Load the pickle file into a DataFrame
df = pd.read_pickle('Project-3_NYC_311_Calls.pkl')

# Set 'Created Date' column as the index and convert it to DatetimeIndex
df = df.set_index(pd.DatetimeIndex(df['Created Date']))
del df['Created Date']  # Remove redundant column after setting index

# Filter the DataFrame to include only data for the year 2022
df_2022 = df[(df.index >= '2022-01-01') & (df.index < '2023-01-01')]  # Filtering for the year 2022

# Resample the data on a daily basis and count the number of complaints each day
daily_complaints_2022 = df_2022.resample('D')['Unique Key'].count()

# Calculate the average number of daily complaints received in 2022
average_daily_complaints_2022 = daily_complaints_2022.mean()

print("Average number of daily complaints received in 2022:", average_daily_complaints_2022)
Average number of daily complaints received in 2022: 8684.320547945206


## Maximum number of calls received on a single day
max_calls_date = df['Unique Key'].resample('D').count().idxmax()
print(f"Date with maximum number of calls: {max_calls_date}")
Date with maximum number of calls: 2020-08-04 00:00:00


## The most important complaint type on the date the maximum number of calls were received

# Find the date with the maximum number of calls
max_calls_date = df['Unique Key'].resample('D').count().idxmax()

# Filter the dataset for that specific date and find the most common complaint type
most_common_complaint = df.loc[df.index.date == max_calls_date.date()]['Complaint Type'].value_counts().idxmax()
print(f"Most common complaint on {max_calls_date.date()}: {most_common_complaint}")

Most common complaint on 2020-08-04: Damaged Tree


## Quietest month
quietest_month = df['Unique Key'].resample('ME').count().idxmin().strftime("%b")
print(f"Quietest month historically: {quietest_month}")

Quietest month historically: Aug

## The value of the seasonal component on 2020-12-25
# Resample your time series to a daily frequency.  Perform ETS decomposition based on an additive model. 
from statsmodels.tsa.seasonal import seasonal_decompose

# Resample the series to a daily frequency
daily_series = df['Unique Key'].resample('D').count()

# Perform ETS decomposition based on an additive model
result = seasonal_decompose(daily_series, model='additive')

# Get the seasonal component value on 2020-12-25, rounded to the nearest integer
seasonal_component = result.seasonal['2020-12-25'].round()
print(f"Seasonal component on 2020-12-25: {seasonal_component}")

Seasonal component on 2020-12-25: 183.0

## The autocorrelation of the number of daily calls with the number of calls the day prior
# Calculate the autocorrelation with a lag of 1
autocorrelation = daily_series.autocorr(lag=1)
print(f"Autocorrelation with lag of 1: {autocorrelation}")
Autocorrelation with lag of 1: 0.7517059728398577

## Forecast the daily series with a test set of 90 days using the Prophet library
from prophet import Prophet
from sklearn.metrics import mean_squared_error
import numpy as np

# Prepare the data for Prophet (requires 'ds' and 'y' columns)
# Ensure that 'ds' is datetime and 'y' is the column with values
prophet_df = daily_series.reset_index()
prophet_df.columns = ['ds', 'y']

# Convert 'ds' to datetime if it's not already
prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])

# Split the data into training and test sets (last 90 days as test set)
train = prophet_df.iloc[:-90]
test = prophet_df.iloc[-90:]

# Initialize and fit the Prophet model
model = Prophet()
model.fit(train)

# Make future dataframe for the next 90 days and predict
future = model.make_future_dataframe(periods=90)
forecast = model.predict(future)

# Calculate RMSE on the test set
test_forecast = forecast.set_index('ds').loc[test['ds']]
rmse = np.sqrt(mean_squared_error(test['y'], test_forecast['yhat']))
print(f"RMSE on test set: {rmse}")
from prophet import Prophet
from sklearn.metrics import mean_squared_error
import numpy as np

# Prepare the data for Prophet (requires 'ds' and 'y' columns)
# Ensure that 'ds' is datetime and 'y' is the column with values
prophet_df = daily_series.reset_index()
prophet_df.columns = ['ds', 'y']

# Convert 'ds' to datetime if it's not already
prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])

# Split the data into training and test sets (last 90 days as test set)
train = prophet_df.iloc[:-90]
test = prophet_df.iloc[-90:]

# Initialize and fit the Prophet model
model = Prophet()
model.fit(train)

# Make future dataframe for the next 90 days and predict
future = model.make_future_dataframe(periods=90)
forecast = model.predict(future)

# Calculate RMSE on the test set
test_forecast = forecast.set_index('ds').loc[test['ds']]
rmse = np.sqrt(mean_squared_error(test['y'], test_forecast['yhat']))
/opt/conda/envs/mggy8413v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
20:06:46 - cmdstanpy - INFO - Chain [1] start processing
20:06:48 - cmdstanpy - INFO - Chain [1] done processing

RMSE on test set: 1233.7823321393885
